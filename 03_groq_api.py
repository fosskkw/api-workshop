import os
import requests
from dotenv import load_dotenv  # For loading environment variables from .env file

# Load the environment variables from the .env file at the root of the project.
# .env files are not tracked inside git (you need to explicitly specify that inside
# the .gitignore of your project) and let us store secrets like API keys without
# exposing them.
load_dotenv()

# Define the base URL for the Groq API (which follows OpenAI's API format)
GROQ_API = "https://api.groq.com/openai/v1"

def call_llm(prompt):
    """
    Sends a prompt to the Groq LLM API and returns the response
    """
    # Construct the endpoint URL for chat completions
    endpoint = f"{GROQ_API}/chat/completions"
    
    # Get the API key from environment variables
    api_key = os.environ["GROQ_KEY"]

    # Make a POST request to the Groq API
    response = requests.post(
        endpoint,
        headers={
            "Content-Type": "application/json",  # Specify we're sending JSON data
            "Authorization": f"Bearer {api_key}",  # Authentication using Bearer token
        },
        json={
            "model": "llama-3.1-8b-instant",  # Specify which LLM to use (Llama 3.1 8B)
            "messages": [
                {
                    "role": "user",  # Message is from the user
                    "content": prompt,  # The actual text prompt
                },
            ],
        },
    )

    # Return the JSON response
    return response.json()

# Call the LLM with a simple prompt asking about APIs
# Extract the generated text from the nested response structure:
# - 'choices' is an array of possible responses (usually just one)
# - [0] gets the first (and typically only) response
# - 'message' contains the response message object
# - 'content' contains the actual text generated by the LLM
answer = call_llm("Explain in short, what is an API?")['choices'][0]['message']['content']

# Print the LLM's response
print(answer)
